{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (emb): Embeddings(\n",
       "    (token_embedding): Embedding(50257, 512)\n",
       "    (positional_embedding): Embedding(128, 512)\n",
       "    (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from model.gpt import GPT\n",
    "import tiktoken\n",
    "from utils.sampling import top_k_logits\n",
    "from generate import generate_text\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    \n",
    "\n",
    "model_cfg = config[\"model\"]\n",
    "train_cfg = config[\"training\"]\n",
    "data_cfg  = config[\"data\"]\n",
    "\n",
    "dataset_path = config[\"data\"]['dataset_path']\n",
    "vocab_size = model_cfg['vocab_size']\n",
    "max_T = model_cfg['max_T']\n",
    "d_model = model_cfg['d_model']\n",
    "n_heads = model_cfg['n_heads']\n",
    "d_ff = model_cfg['d_ff']\n",
    "n_layers = model_cfg['n_layers']\n",
    "NEG_INF = -1e5\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "model = GPT(d_model, vocab_size, max_T, n_layers, n_heads, d_ff)\n",
    "\n",
    "ckpt = torch.load(\"checkpoint.pt\", map_location=device) \n",
    "model.load_state_dict(ckpt[\"model\"]) \n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== temperature = 0.5 ===\n",
      "Once upon a time, a team was in the top of the final.\n",
      "\n",
      "“I’m going to be a good thing.”\n",
      "\n",
      "“We’ve got to get a lot of things like,” said W. “I’m not going to do.”\n",
      "\n",
      "“I’ve been a lot of years,” he\n",
      "\n",
      "=== temperature = 0.8 ===\n",
      "Once upon a time, the new company will have to have to be paid against the best, and we will be able to be able to move.\n",
      "\n",
      "The first time you will get a new version of the final day (and they’d read this will be below), the first time he’s going to win and make it a couple of days to be on the first game.\n",
      "\n",
      "�\n",
      "\n",
      "=== temperature = 1.0 ===\n",
      "Once upon a time.\n",
      "\n",
      "“If I’m good for a very little bit of thing, a lot of what you are going to feel. I’d like your life.\n",
      "\n",
      "“The same is a lot of years.”\n",
      "\n",
      "Hutzan said as much as the most famous of you say that the entire thing is to do, is very efficient, so\n",
      "\n",
      "=== temperature = 1.3 ===\n",
      "Once upon a time later, the way we have to ask him. I see it to say that it must come to do.”\n",
      "\n",
      "It’s a good idea of his work when Trump took a position when he was “stibling” in favor of the U-S’s work, and just before it had come as to have the right to the wrong game, it\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "\n",
    "for t in [0.5, 0.8, 1.0, 1.3]:\n",
    "    print(f\"\\n=== temperature = {t} ===\")\n",
    "    print(generate_text(\n",
    "        model,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=80,\n",
    "        temperature=t,\n",
    "        top_k=40,\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [None, 20, 40, 100]:\n",
    "    print(f\"\\n=== top_k = {k} ===\")\n",
    "    print(generate_text(\n",
    "        model,\n",
    "        prompt=\"The future of AI is\",\n",
    "        max_new_tokens=80,\n",
    "        temperature=1.0,\n",
    "        top_k=k,\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    return generate_text(\n",
    "        model,\n",
    "        prompt=\"Deep learning models\",\n",
    "        max_new_tokens=60,\n",
    "        temperature=1.0,\n",
    "        top_k=40,\n",
    "    )\n",
    "\n",
    "print(run_with_seed(42))\n",
    "print(run_with_seed(42))  # identyczne\n",
    "print(run_with_seed(123)) # inne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short = \"AI will change the world.\"\n",
    "long = \"AI \" * 200\n",
    "\n",
    "print(generate_text(model, short, 50, 1.0, 40))\n",
    "print(generate_text(model, long, 50, 1.0, 40))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
